# Plot sum vs product kernel predictions on same figure
plot(times, temp_prod,
type = "o",
pch = 19,
col = "black",
lwd = 2,
xlab = "Hour of the day",
ylab = "Predicted Temperature (°C)",
main = "Temperature forecast",
ylim = range(c(temp_sum, temp_prod), na.rm = TRUE),
xaxt = "n")  # suppress x-axis to format manually
# Add sum-kernel predictions
points(times, temp_sum,
type = "o",
pch = 17,
col = "red",
lwd = 2)
# Format x-axis
axis(1, at = times, labels = paste0(times, ":00"))
# Add light grid
grid(nx = NA, ny = NULL, col = "lightgray", lty = "dotted")
# Add legend
legend("topright",
legend = c("Kernel product", "Kernel sum"),
col = c("black", "red"),
pch = c(19, 17),
lwd = 2)
# oral defence: comparison of sum vs product of kernels:
# Using the sum of kernels gives smooth temperature predictions because a measurement
# gets high weight if it is close in any one of the three dimensions (space, date, or time).
# Using the product of kernels gives more variable predictions because a measurement
# only contributes significantly if it is close in all three dimensions simultaneously.
# Therefore, the sum kernel smooths over gaps in data, while the product kernel emphasizes
# only the most relevant nearby measurements, leading to sharper peaks and dips.
# Lab 3 block 1 of 732A99/TDDE01/732A68 Machine Learning
# Author: jose.m.pena@liu.se
# Made for teaching purposes
library(kernlab)
set.seed(1234567890)
data(spam)
foo <- sample(nrow(spam))
spam <- spam[foo,]
tr <- spam[1:3000, ]
va <- spam[3001:3800, ]
trva <- spam[1:3800, ]
te <- spam[3801:4601, ]
by <- 0.3
err_va <- NULL
for(i in seq(by,5,by)){
filter <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=i,scaled=FALSE)
mailtype <- predict(filter,va[,-58])
t <- table(mailtype,va[,58])
err_va <-c(err_va,(t[1,2]+t[2,1])/sum(t))
}
filter0 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter0,va[,-58])
t <- table(mailtype,va[,58])
err0 <- (t[1,2]+t[2,1])/sum(t)
err0
filter1 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter1,te[,-58])
t <- table(mailtype,te[,58])
err1 <- (t[1,2]+t[2,1])/sum(t)
err1
filter2 <- ksvm(type~.,data=trva,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter2,te[,-58])
t <- table(mailtype,te[,58])
err2 <- (t[1,2]+t[2,1])/sum(t)
err2
filter3 <- ksvm(type~.,data=spam,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter3,te[,-58])
t <- table(mailtype,te[,58])
err3 <- (t[1,2]+t[2,1])/sum(t)
err3
# Questions
# 1. Which filter do we return to the user ? filter0, filter1, filter2 or filter3? Why?
# 2. What is the estimate of the generalization error of the filter returned to the user? err0, err1, err2 or err3? Why?
# 3. Implementation of SVM predictions.
sv<-alphaindex(filter3)[[1]]
co<-coef(filter3)[[1]]
inte<- - b(filter3)
k<-NULL
for(i in 1:10){ # We produce predictions for just the first 10 points in the dataset.
k2<-NULL
for(j in 1:length(sv)){
k2<- # Your code here
}
filter0
filter1
filter2
filter3
va
colnames(va)
colnames(va)[58]
va["type"]
library(pracma)
library(neuralnet)
set.seed(12345)
help(runif)
library(pracma)
library(neuralnet)
set.seed(12345)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
help(neuralnet)
mydata
colnames(mydata)
mydata$Sin
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
ponts(te[,1], predict(nn, te), col="red", cex=1)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
plot(tr, cex=2)
length(tr)
dim(tf)
dim(tr)
dim(tr[,0])
length(tr[,0])
tr[,0]
tr[0,]
tr[2,0]
tr[2,]
nrow(tf)
nrow(tr)
nrow(te)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
summary(nn)
help(runif)
winit <- runif(10, min=-1,max=1)
help("neuralnet")
winit <- runif(10, min=-1,max=1)
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
pred_nn <- preict(nn, te)
pred_nn <- predict(nn, newdata=te)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
summary(pred_nn)
help("neuralnet")
linear_activation <- function(x){
return x
linear_activation <- function(x){
x
}
linear_activation(5)
linear_activation <- function(x){
x
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = linear_activation)
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = linear_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
max(0,x)
max(0,2)
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = ReLU_activation)
# activation function: ReLU
ReLU_activation <- function(x){
max(0,x)
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = ReLU_activation)
# activation function: ReLU
ReLU_activation <- function(x){
ifelse(x > 0, x, 0)
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = ReLU_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
# activation function: ReLU
ReLU_activation <- function(x){
ifelse(x > 0, x, 0)
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = ReLU_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
ln(e)
log(1)
log(e)
log(2.7)
exp(log(2.4))
# activation function: softplus
softplus_activation <- function(x){
log(1 + exp(x))
}
# activation function: softplus
softplus_activation <- function(x){
log(1 + exp(x))
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = softplus_activation())
# activation function: softplus
softplus_activation <- function(x){
log(1 + exp(x))
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = softplus_activation)
library(pracma)
library(neuralnet)
#TASK 1
set.seed(12345)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
winit <- runif(10, min=-1,max=1)
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
#results are good. as graph shows
#TASK 2 - change with custom activation function
#linear activation function
linear_activation <- function(x){
x
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = linear_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1) #really bad, straight line
# activation function: ReLU
ReLU_activation <- function(x){
ifelse(x > 0, x, 0)
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = ReLU_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1) #slightly better but still really bad
# activation function: softplus
softplus_activation <- function(x){
log(1 + exp(x))
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = softplus_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1) #slightly better but still really bad
nn
library(pracma)
library(neuralnet)
#TASK 1
set.seed(12345)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
winit <- runif(10, min=-1,max=1)
nn_orig <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
library(pracma)
library(neuralnet)
#TASK 1
set.seed(12345)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
winit <- runif(10, min=-1,max=1)
nn_orig <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1)
library(pracma)
library(neuralnet)
#TASK 1
set.seed(12345)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
winit <- runif(10, min=-1,max=1)
nn_orig <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn_orig, te), col="red", cex=1)
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = softplus_activation, linear.output = TRUE)
# activation function: softplus
softplus_activation <- function(x){
log(1 + exp(x))
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = softplus_activation, linear.output = TRUE)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1) #slightly better but still really bad
set.seed(1234567890)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
winit <- runif(10, min=-1,max=1)
nn_orig <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn_orig, te), col="red", cex=1)
#results are good. as graph shows
#TASK 2 - change with custom activation function
#linear activation function
linear_activation <- function(x){
x
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = linear_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1) #really bad, straight line
# activation function: ReLU
ReLU_activation <- function(x){
ifelse(x > 0, x, 0)
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = ReLU_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1) #slightly better but still really bad
# activation function: softplus
softplus_activation <- function(x){
log(1 + exp(x))
}
nn <- neuralnet(Sin ~ ., data=mydata, hidden=10, startweights = winit, act.fct = softplus_activation, linear.output = TRUE)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1) #slightly better but still really bad
library(pracma)
library(neuralnet)
#TASK 1
set.seed(1234567890)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
winit <- runif(10, min=-1,max=1)
nn_orig <- neuralnet(Sin ~ ., data=tr, hidden=10, startweights = winit)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn_orig, te), col="red", cex=1)
#results are good. as graph shows
#TASK 2 - change with custom activation function
#linear activation function
linear_activation <- function(x){
x
}
nn <- neuralnet(Sin ~ ., data=tr, hidden=10, startweights = winit, act.fct = linear_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1) #really bad, straight line
# activation function: ReLU
ReLU_activation <- function(x){
ifelse(x > 0, x, 0)
}
nn <- neuralnet(Sin ~ ., data=tr, hidden=10, startweights = winit, act.fct = ReLU_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1) #slightly better but still really bad
# activation function: softplus
softplus_activation <- function(x){
log(1 + exp(x))
}
nn <- neuralnet(Sin ~ ., data=tr, hidden=10, startweights = winit, act.fct = softplus_activation, linear.output = TRUE)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn, te), col="red", cex=1) #slightly better but still really bad
#linear activation function
linear_activation <- function(x){
x
}
nn_linear <- neuralnet(Sin ~ ., data=tr, hidden=10, startweights = winit, act.fct = linear_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn_linear, te), col="red", cex=1) #really bad, straight line
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn_softplus, te), col="red", cex=1) #slightly better but still really bad
nn_softplus <- neuralnet(Sin ~ ., data=tr, hidden=10, startweights = winit, act.fct = softplus_activation, linear.output = TRUE)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn_softplus, te), col="red", cex=1) #slightly better but still really bad
nn_relu <- neuralnet(Sin ~ ., data=tr, hidden=10, startweights = winit, act.fct = ReLU_activation)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1], predict(nn_relu, te), col="red", cex=1) #slightly better but still really bad
Var_new <- runif(500, 0, 50)
Var_new <- runif(500, 0, 50)
Var_new <- runif(500, 0, 50)
newdata <- data.frame(Var_new, Sin=sin(Var_new))
plot(tr, cex=2)
points(newdata, col = "blue", cex=1)
points(te[,1], predict(nn_orig, newdata), col="red", cex=1) #really bad, straight line
te
colnames(te)
colnames(newdata)
Var <- runif(500, 0, 50)
newdata <- data.frame(Var, Sin=sin(Var_new))
plot(tr, cex=2)
points(newdata, col = "blue", cex=1)
points(te[,1], predict(nn_orig, newdata), col="red", cex=1) #really bad, straight line
#TASK 3
Var <- runif(500, 0, 50)
newdata <- data.frame(Var, Sin=sin(Var))
plot(tr, cex=2)
points(newdata, col = "blue", cex=1)
points(te[,1], predict(nn_orig, newdata), col="red", cex=1) #really bad, straight line
Var <- runif(500, 0, 50)
newdata <- data.frame(Var, Sin=sin(Var))
plot(tr, cex=2)
points(newdata[,1], col = "blue", cex=1)
points(te[,1], predict(nn_orig, newdata), col="red", cex=1) #really bad, straight line
#TASK 3
Var <- runif(500, 0, 50)
newdata <- data.frame(Var, Sin=sin(Var))
plot(tr, cex=2)
points(newdata[,1], col = "blue", cex=1)
points(te[,1], predict(nn_orig, newdata[1]), col="red", cex=1) #really bad, straight line
# TASK 3 - test extrapolation capability
Var <- runif(500, 0, 50)
newdata <- data.frame(Var, Sin=sin(Var))
# Plot: Show how the network trained on [0,10] fails on [0,50]
plot(tr, cex=2, xlab="Var", ylab="Sin", xlim=c(0, 50), ylim=c(-1, 1),
main="Neural Network Extrapolation Failure")
points(newdata, col="blue", cex=0.5)  # True sine values in [0,50]
points(newdata[,1], predict(nn_orig, newdata), col="red", cex=0.5)  # Predictions
legend("bottomright", c("train data [0,10]", "true sin [0,50]", "predictions"),
col=c("black", "blue", "red"), pch=1, cex=0.8)
nn_orig$weights
# TASK 3 - test extrapolation capability
Var <- runif(500, 0, 50)
newdata <- data.frame(Var, Sin=sin(Var))
# Plot: Show how the network trained on [0,10] fails on [0,50]
plot(tr, cex=2, xlab="Var", ylab="Sin", xlim=c(0, 50), ylim=c(-1, 1),
main="Neural Network Extrapolation Failure")
points(newdata, col="blue", cex=0.5)  # True sine values in [0,50]
points(newdata[,1], predict(nn_orig, newdata), col="red", cex=0.5)  # Predictions
legend("bottomright", c("train data [0,10]", "true sin [0,50]", "predictions"),
col=c("black", "blue", "red"), pch=1, cex=0.8)
cat("Prediction range:", range(predictions), "\n")
new_pred <- predict(nn_orig, newdata)
new_pred <- predict(nn_orig, newdata)
# Plot: Show how the network trained on [0,10] fails on [0,50]
plot(tr, cex=2, xlab="Var", ylab="Sin", xlim=c(0, 50), ylim=c(-1, 1),
main="Neural Network Extrapolation Failure")
points(newdata, col="blue", cex=0.5)  # True sine values in [0,50]
points(newdata[,1], new_pred, col="red", cex=0.5)  # Predictions
legend("bottomright", c("train data [0,10]", "true sin [0,50]", "predictions"),
col=c("black", "blue", "red"), pch=1, cex=0.8)
cat("Prediction range:", range(new_pred), "\n")
cat("True sin range:", range(newdata$Sin), "\n")
#TASK 4
new_pred
which(pred_new < -0.99)
which(new_pred < -0.99)
# TASK 3 - test extrapolation capability with dynamic scaling
Var <- runif(500, 0, 50)
newdata <- data.frame(Var, Sin=sin(Var))
predictions <- predict(nn_orig, newdata)
# Check the ranges
cat("Prediction range:", range(predictions), "\n")
cat("True sin range:", range(newdata$Sin), "\n")
# Determine y-axis limits to include all data
y_min <- min(newdata$Sin, predictions)
y_max <- max(newdata$Sin, predictions)
# Plot with automatic y-axis scaling
plot(tr, cex=2, xlab="Var", ylab="Sin", xlim=c(0, 50),
ylim=c(y_min, y_max),
main="Neural Network Extrapolation Failure")
points(newdata, col="blue", cex=0.5)  # True sine values in [0,50]
points(newdata[,1], predictions, col="red", cex=0.5)  # Predictions
# Add a vertical line to show training boundary
abline(v=10, lty=2, col="gray", lwd=2)
# Add a vertical line to show training boundary
abline(v=10, lty=2, col="gray", lwd=2)
text(10, y_max*0.9, "Trainingboundary", pos=4, col="gray")
legend("bottomright", c("train data [0,10]", "true sin [0,50]", "predictions"),
col=c("black", "blue", "red"), pch=1, cex=0.8)
nn_orig$weights
# See what happens to one neuron
Var_values <- c(0, 5, 10, 20, 50)
for(v in Var_values) {
activation <- 1 / (1 + exp(-(11.96 - 1.80 * v)))
cat("Var =", v, "→ activation =", round(activation, 6), "\n")
}
# TASK 5 predict x from sin(x) (inverse problem)
set.seed(1234567890)
Var <- runif(500, 0, 10)
mydata <- data.frame(Sin=sin(Var), Var=Var)  # Sin is input, Var is output!
# Train on ALL 500 points
winit <- runif(10, -1, 1)
# Train the network - may need threshold to avoid convergence issues
nn_inverse <- neuralnet(Var ~ Sin, data=mydata, hidden=10,
startweights=winit, threshold=0.1)
# Predict
predictions <- predict(nn_inverse, mydata)
# Plot results
plot(mydata$Sin, mydata$Var, col="blue", cex=0.8,
xlab="sin(x)", ylab="x",
main="Predicting x from sin(x) - The Inverse Problem")
points(mydata$Sin, predictions, col="red", cex=0.8)
legend("topleft", c("True x", "Predicted x"),
col=c("blue", "red"), pch=1, cex=0.8)
# Calculate error
mse <- mean((mydata$Var - predictions)^2)
cat("Mean Squared Error:", mse, "\n")
