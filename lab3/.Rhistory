setwd("C:/Users/Manfred/Desktop/tdde01/ml-lab-tdde01/lab3")
set.seed(12345)
library(geosphere)
library("dplyr")
stations <- read.csv("stations.csv", fileEncoding = "latin1")
temperatures <- read.csv("temps50k.csv")
st <- merge(stations, temperatures, by="station_number")
#date and time pre-processing
st$date <- as.Date(st$date)
st$time <- substr(st$time, 1, 5)
st$hour <- as.numeric(substr(st$time, 1, 2))
#position pre-processing, calculating haversine distance
positions <- stations%>%select(longitude, latitude)
h_space <- 80000
h_date <- 20
h_time <- 2
#a <- 67.91130 remote point
#b <-  19.60680
#
# a <- 59.8953
# b <- 17.5935# high freq
a <- 58.0340 # Lat
b <- 14.9756 # Long external
# a <-58.4274 #59.8953   17.5935 high frequency point
# b <- 14.826
space_distances <- distHaversine(positions, c(b,a))
#date <- as.Date("2013-07-04") #date to predict
date <- as.Date("2013-08-16")
times <- c(
"04:00:00",
"06:00:00",
"08:00:00",
"10:00:00",
"12:00:00",
"14:00:00",
"16:00:00",
"18:00:00",
"20:00:00",
"22:00:00",
"24:00:00"
)
times <- as.numeric(substr(times, 1, 2))
temp_sum <- vector(length = length(times))     # sum of kernels
temp_prod <- vector(length = length(times))    # product of kernels
#scaled_distance <- function(data, prediction, smoothing) (data-prediction)/h
gaussian_kernel <- function(u) exp(-u^2)
for(i in seq_along(times)){
pred_time <- times[i]
st_filtered <- st %>%
filter(date < date | (date == date & hour <= pred_time))
#calculate the distances in each domain
d_space <- distHaversine(cbind(st_filtered$longitude, st_filtered$latitude), c(b,a))
d_date <- abs(as.numeric(st_filtered$date - date))
d_time <- abs(st_filtered$hour - pred_time)
#scale the distances
u_space <- d_space/h_space
u_date <- d_date/h_date
u_time <- d_time/h_time
#calculate the kernel values
k_space <- gaussian_kernel(u_space)
k_date <- gaussian_kernel(u_date)
k_time <- gaussian_kernel(u_time)
w_sum <- k_space + k_date + k_time
w_prod <- k_space * k_date * k_time #product of kernels
temp_sum[i] <- sum(w_sum * st_filtered$air_temperature) / sum(w_sum)
temp_prod[i] <- sum(w_prod * st_filtered$air_temperature) / sum(w_prod)
}
# Plot sum vs product kernel predictions on same figure
plot(times, temp_prod,
type = "o",
pch = 19,
col = "black",
lwd = 2,
xlab = "Hour of the day",
ylab = "Predicted Temperature (째C)",
main = "Temperature forecast (Median Clanker)",
ylim = range(c(temp_sum, temp_prod), na.rm = TRUE),
xaxt = "n")  # suppress x-axis to format manually
# Add sum-kernel predictions
points(times, temp_sum,
type = "o",
pch = 17,
col = "red",
lwd = 2)
# Format x-axis
axis(1, at = times, labels = paste0(times, ":00"))
# Add light grid
grid(nx = NA, ny = NULL, col = "lightgray", lty = "dotted")
# Add legend
legend("topright",
legend = c("Kernel product", "Kernel sum"),
col = c("black", "red"),
pch = c(19, 17),
lwd = 2)
# oral defence: comparison of sum vs product of kernels:
# Using the sum of kernels gives smooth temperature predictions because a measurement
# gets high weight if it is close in any one of the three dimensions (space, date, or time).
# Using the product of kernels gives more variable predictions because a measurement
# only contributes significantly if it is close in all three dimensions simultaneously.
# Therefore, the sum kernel smooths over gaps in data, while the product kernel emphasizes
# only the most relevant nearby measurements, leading to sharper peaks and dips.
# Plot sum vs product kernel predictions on same figure
plot(times, temp_prod,
type = "o",
pch = 19,
col = "black",
lwd = 2,
xlab = "Hour of the day",
ylab = "Predicted Temperature (째C)",
main = "Temperature forecast",
ylim = range(c(temp_sum, temp_prod), na.rm = TRUE),
xaxt = "n")  # suppress x-axis to format manually
# Add sum-kernel predictions
points(times, temp_sum,
type = "o",
pch = 17,
col = "red",
lwd = 2)
# Plot sum vs product kernel predictions on same figure
plot(times, temp_prod,
type = "o",
pch = 19,
col = "black",
lwd = 2,
xlab = "Hour of the day",
ylab = "Predicted Temperature (째C)",
main = "Temperature forecast (Median Clank",
ylim = range(c(temp_sum, temp_prod), na.rm = TRUE),
xaxt = "n")  # suppress x-axis to format manually
set.seed(12345)
library(geosphere)
library("dplyr")
stations <- read.csv("stations.csv", fileEncoding = "latin1")
temperatures <- read.csv("temps50k.csv")
st <- merge(stations, temperatures, by="station_number")
#date and time pre-processing
st$date <- as.Date(st$date)
st$time <- substr(st$time, 1, 5)
st$hour <- as.numeric(substr(st$time, 1, 2))
#position pre-processing, calculating haversine distance
positions <- stations%>%select(longitude, latitude)
h_space <- 80000
h_date <- 20
h_time <- 2
#a <- 67.91130 remote point
#b <-  19.60680
#
# a <- 59.8953
# b <- 17.5935# high freq
a <- 58.0340 # Lat
b <- 14.9756 # Long external
# a <-58.4274 #59.8953   17.5935 high frequency point
# b <- 14.826
space_distances <- distHaversine(positions, c(b,a))
#date <- as.Date("2013-07-04") #date to predict
date <- as.Date("2013-08-16")
times <- c(
"04:00:00",
"06:00:00",
"08:00:00",
"10:00:00",
"12:00:00",
"14:00:00",
"16:00:00",
"18:00:00",
"20:00:00",
"22:00:00",
"24:00:00"
)
times <- as.numeric(substr(times, 1, 2))
temp_sum <- vector(length = length(times))     # sum of kernels
temp_prod <- vector(length = length(times))    # product of kernels
#scaled_distance <- function(data, prediction, smoothing) (data-prediction)/h
gaussian_kernel <- function(u) exp(-u^2)
for(i in seq_along(times)){
pred_time <- times[i]
st_filtered <- st %>%
filter(date < date | (date == date & hour <= pred_time))
#calculate the distances in each domain
d_space <- distHaversine(cbind(st_filtered$longitude, st_filtered$latitude), c(b,a))
d_date <- abs(as.numeric(st_filtered$date - date))
d_time <- abs(st_filtered$hour - pred_time)
#scale the distances
u_space <- d_space/h_space
u_date <- d_date/h_date
u_time <- d_time/h_time
#calculate the kernel values
k_space <- gaussian_kernel(u_space)
k_date <- gaussian_kernel(u_date)
k_time <- gaussian_kernel(u_time)
w_sum <- k_space + k_date + k_time
w_prod <- k_space * k_date * k_time #product of kernels
temp_sum[i] <- sum(w_sum * st_filtered$air_temperature) / sum(w_sum)
temp_prod[i] <- sum(w_prod * st_filtered$air_temperature) / sum(w_prod)
}
# Plot sum vs product kernel predictions on same figure
plot(times, temp_prod,
type = "o",
pch = 19,
col = "black",
lwd = 2,
xlab = "Hour of the day",
ylab = "Predicted Temperature (째C)",
main = "Temperature forecast",
ylim = range(c(temp_sum, temp_prod), na.rm = TRUE),
xaxt = "n")  # suppress x-axis to format manually
# Add sum-kernel predictions
points(times, temp_sum,
type = "o",
pch = 17,
col = "red",
lwd = 2)
# Format x-axis
axis(1, at = times, labels = paste0(times, ":00"))
# Add light grid
grid(nx = NA, ny = NULL, col = "lightgray", lty = "dotted")
# Add legend
legend("topright",
legend = c("Kernel product", "Kernel sum"),
col = c("black", "red"),
pch = c(19, 17),
lwd = 2)
# oral defence: comparison of sum vs product of kernels:
# Using the sum of kernels gives smooth temperature predictions because a measurement
# gets high weight if it is close in any one of the three dimensions (space, date, or time).
# Using the product of kernels gives more variable predictions because a measurement
# only contributes significantly if it is close in all three dimensions simultaneously.
# Therefore, the sum kernel smooths over gaps in data, while the product kernel emphasizes
# only the most relevant nearby measurements, leading to sharper peaks and dips.
